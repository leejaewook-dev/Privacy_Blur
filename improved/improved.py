#light_system
import os
import re
import cv2
import json
import numpy as np
import torch
from PIL import Image
from ultralytics import YOLO
from facenet_pytorch import MTCNN, InceptionResnetV1
import easyocr
import matplotlib.pyplot as plt

# =========================
# 0) ì„¤ì •
# =========================
# â€” ArcFace ì„ê³„ê°’: ì›ë³¸ ì„ë² ë”©/ë³´ì • ì„ë² ë”© ë¶„ë¦¬ (Patch A)
THRESHOLD      = 0.40  # ì›ë³¸ ì„ë² ë”© ê¸°ì¤€
THRESHOLD_ENH  = 0.43  # ë³´ì • ì„ë² ë”©(í´ë°±)ì¼ ë•Œ ì¡°ê¸ˆ ë” ë³´ìˆ˜ì ìœ¼ë¡œ

# â€” ì „ì—­ ë°ê¸° íŒì •(ë³´ìˆ˜ì ) + ë³´ì • ê°•ë„ (Patch B)
VIDEO_Y_MEAN_THR = 90     # 95â†’90: ì§„ì§œ ì–´ë‘ìš¸ ë•Œë§Œ ë³´ì • ì¼œê¸°
ROI_BORDER       = 0.10   # ì¤‘ì•™ 80% ì˜ì—­ë§Œ ì¸¡ì •(ë ˆí„°ë°•ìŠ¤ ë¬´ì‹œ)
FORCE_GLOBAL_ENH = None   # None=ìë™, True=í•­ìƒ ì¼œê¸°, False=í•­ìƒ ë„ê¸°
GLOBAL_TARGET_Y  = 130    # 150â†’130: ê³¼ë…¸ì¶œ/ë…¸ì´ì¦ˆ í™•ëŒ€ ë°©ì§€

# â€” ì–´ë‘ìš¸ ë•Œë§Œ ì¡°ê±´ë¶€ ì¶”ê°€ ê°œì„  (ê¸°ë³¸ OFF, í•„ìš” ì‹œ ì¼œì„œ A/B í…ŒìŠ¤íŠ¸)
USE_WB                 = False   # Gray-World í™”ì´íŠ¸ë°¸ëŸ°ìŠ¤
USE_ADAPTIVE_SHARPEN   = True   # ì €ì„ ëª… í”„ë ˆì„ ìƒ¤í”ˆ
USE_HIGHLIGHT_COMPRESS = False   # í•˜ì´ë¼ì´íŠ¸ ì••ì¶•

SHARP_THR   = 120.0  # Laplacian Var ì„ê³„ê°’(100~160)
HI_CLIP_THR = 0.02   # Y>=250 ë¹„ìœ¨ì´ 2%â†‘ë©´ í•˜ì´ë¼ì´íŠ¸ ì••ì¶•

# â€” ë“±ë¡ì í‰ê·  ì„ë² ë”©
centroid = np.load(r'C:\Users\User\Desktop\average_centroid_deep_agument.npy')

# ë¹„ë””ì˜¤/GT ê²½ë¡œ (refëŠ” GT ë§Œë“  ê¸°ì¤€ ì˜ìƒ)
video_path       = r'C:\Users\User\Desktop\your_video.mp4'   # í…ŒìŠ¤íŠ¸ ì…ë ¥
ref_video_path   = r'C:\Users\User\Desktop\your_video.mp4'   # GT ê¸°ì¤€
save_video_path  = r'C:\Users\User\Desktop\output_mosaic.mp4'
labels_json_dir  = r'C:\Users\User\Desktop\labels_json'      # âœ… ìˆ˜ë™ GT(JSON)

# ì‚°ì¶œë¬¼ í´ë”
base_dir          = r'C:\Users\User\Desktop'
before_frame_dir  = os.path.join(base_dir, 'before_blur_frames')
after_frame_dir   = os.path.join(base_dir, 'after_blur_frames')
before_face_dir   = os.path.join(base_dir, 'before_blur_faces')
after_face_dir    = os.path.join(base_dir, 'after_blur_faces')
auto_pred_dir     = os.path.join(base_dir, 'auto_pred_json')     # âœ… ìë™ ì˜ˆì¸¡ JSON
enh_video_path    = r'C:\Users\User\Desktop\det_input_video.mp4' # ê²€ì¶œ/ì¸ì‹ ì…ë ¥(ë³´ì •ë³¸) ë¯¸ë¦¬ë³´ê¸°
det_frame_dir     = os.path.join(base_dir, 'det_input_frames')   # (ì˜µì…˜) í”„ë ˆì„ ì €ì¥

for d in [before_frame_dir, after_frame_dir, before_face_dir, after_face_dir, auto_pred_dir, det_frame_dir]:
    os.makedirs(d, exist_ok=True)

# =========================
# 1) ëª¨ë¸ ë¡œë“œ
# =========================
device = 'cuda' if torch.cuda.is_available() else 'cpu'
yolo   = YOLO(r'C:\Users\User\Desktop\yolov8n-seg.pt')  # seg ê¶Œì¥ (person=0)
mtcnn  = MTCNN(image_size=224, margin=40, device=device)
resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)
ocr    = easyocr.Reader(['ko', 'en'])

# =========================
# 2) ìœ í‹¸
# =========================
def mosaic_mask(img, mask, scale=0.05):
    """YOLO seg mask(ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤)ë¡œ ëª¨ìì´í¬"""
    h, w = img.shape[:2]
    resized_mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)
    small_w = max(1, int(w * scale))
    small_h = max(1, int(h * scale))
    small  = cv2.resize(img, (small_w, small_h))
    mosaic = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)
    mask_3d = (resized_mask > 0.5).astype(np.uint8)[:, :, None]
    img[:] = img * (1 - mask_3d) + mosaic * mask_3d

def blur_polygon(image, polygon, ksize=(51, 51)):
    pts = np.array(polygon, dtype=np.int32)
    mask = np.zeros(image.shape[:2], dtype=np.uint8)
    cv2.fillPoly(mask, [pts], 255)
    blurred = cv2.GaussianBlur(image, ksize, 0)
    mask_3ch = cv2.merge([mask] * 3)
    return np.where(mask_3ch == 255, blurred, image)

def is_sensitive_text(text):
    patterns = [
        r'\d{6}-\d{7}', r'01[0-9]-\d{3,4}-\d{4}',
        r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+',
        r'\d{1,4}ë™|\d{1,4}í˜¸', r'[\uac00-\ud7a3]+[ì‹œêµ°êµ¬ë™ìë©´ë¡œê¸¸]',
        r'[\uac00-\ud7a3]{2,20}(ì•„íŒŒíŠ¸|ë¹Œë¼|ì£¼íƒ|ë§¨ì…˜|ì˜¤í”¼ìŠ¤í…”|ì—°ë¦½)',
        r'\d{1,4}-\d{1,4}', r'\d{2,4}-\d{2,4}-\d{4,7}', r'\d{9,14}',
        r'(ëŒ€í•™êµ|ì¤‘í•™êµ|ê³ ë“±í•™êµ|íšŒì‚¬|ì§ì¥|ì†Œì†)',
        r'(ì´ë¦„|ì„±ëª…)[:ï¼š]?\s?[ê°€-í£]{2,4}'
    ]
    s = text.replace(" ", "")
    return any(re.search(p, s) for p in patterns)

def cos_sim(a, b):
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10))

def load_frame_json(frame_idx):
    jf = os.path.join(labels_json_dir, f"frame_{frame_idx:03d}.json")
    if not os.path.exists(jf):
        return None
    with open(jf, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data.get("objects", [])

def center_in_bbox(cx, cy, bbox):
    x1, y1, x2, y2 = bbox
    return (x1 <= cx <= x2) and (y1 <= cy <= y2)

# ----- ROI/ì§€í‘œ ì¸¡ì • -----
def central_roi(bgr, border=ROI_BORDER):
    h, w = bgr.shape[:2]
    y0 = int(h * border); y1 = h - y0
    x0 = int(w * border); x1 = w - x0
    return bgr[y0:y1, x0:x1]

def measure_metrics(bgr):
    roi = central_roi(bgr)
    y = cv2.cvtColor(roi, cv2.COLOR_BGR2YCrCb)[:, :, 0].astype(np.float32)
    y_mean = float(y.mean())
    hi_clip = float((y >= 250).mean())
    sharp = float(cv2.Laplacian(cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY), cv2.CV_64F).var())
    means = roi.mean(axis=(0,1))  # BGR
    b,g,r = means
    rg = float(r/(g+1e-6)); bg = float(b/(g+1e-6))
    return y_mean, hi_clip, sharp, rg, bg

# ----- ê°œì„  í•¨ìˆ˜ë“¤ -----
def gray_world_wb(bgr):
    b,g,r = [bgr[:,:,i].astype(np.float32) for i in range(3)]
    mean_b, mean_g, mean_r = b.mean(), g.mean(), r.mean()
    mean_gray = (mean_b + mean_g + mean_r) / 3.0
    kb, kg, kr = mean_gray/(mean_b+1e-6), mean_gray/(mean_g+1e-6), mean_gray/(mean_r+1e-6)
    out = bgr.copy().astype(np.float32)
    out[:,:,0] = np.clip(b*kb, 0, 255)
    out[:,:,1] = np.clip(g*kg, 0, 255)
    out[:,:,2] = np.clip(r*kr, 0, 255)
    return out.astype(np.uint8)

def unsharp_mask(bgr, sigma=1.0, amount=0.6, thresh=0):
    blur = cv2.GaussianBlur(bgr, (0,0), sigmaX=sigma, sigmaY=sigma)
    sharp = cv2.addWeighted(bgr, 1+amount, blur, -amount, 0)
    if thresh > 0:
        low_contrast_mask = np.abs(bgr.astype(np.int16)-blur.astype(np.int16)).max(axis=2) < thresh
        sharp[low_contrast_mask] = bgr[low_contrast_mask]
    return sharp

def highlight_compress(bgr):
    x = bgr.astype(np.float32)/255.0
    knee = 0.85; roll = 0.10
    y = np.where(x < knee, x, knee + (1 - np.exp(-(x-knee)/roll)) * (1-knee))
    return np.clip(y*255.0, 0, 255).astype(np.uint8)

# ----- ì „ì—­ ë³´ì • (ê°ë§ˆ+CLAHE) -----
def global_enhance(bgr, target_y=GLOBAL_TARGET_Y):
    ycrcb = cv2.cvtColor(bgr, cv2.COLOR_BGR2YCrCb)
    y0 = ycrcb[:, :, 0].astype(np.uint8)
    m = max(1.0, float(y0.mean()))
    gamma = np.log(target_y / 255.0) / np.log(m / 255.0)   # gamma<1 ë°ì•„ì§
    gamma = float(np.clip(gamma, 0.6, 1.6))
    table = np.array([(i/255.0)**gamma * 255 for i in range(256)], dtype=np.uint8)
    y1 = cv2.LUT(y0, table)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    y2 = clahe.apply(y1)
    ycrcb[:, :, 0] = y2
    return cv2.cvtColor(ycrcb, cv2.COLOR_YCrCb2BGR)

# ----- ê²€ì¶œ/ì¸ì‹ ì…ë ¥ìš© íŒŒì´í”„ë¼ì¸ (GLOBAL_ENH=True ë•Œë§Œ ì¶”ê°€ ê°œì„ ) -----
def build_det_input(bgr, global_enh_flag):
    decisions = {"wb": False, "enh": False, "sharpen": False, "hicomp": False}
    out = bgr

    if global_enh_flag:
        if USE_WB:
            out = gray_world_wb(out); decisions["wb"] = True
        out = global_enhance(out, target_y=GLOBAL_TARGET_Y); decisions["enh"] = True

        _, _, sharp, _, _ = measure_metrics(out)
        if USE_ADAPTIVE_SHARPEN and sharp < SHARP_THR:
            out = unsharp_mask(out, sigma=1.0, amount=0.6); decisions["sharpen"] = True

        _, hi_clip, _, _, _ = measure_metrics(out)
        if USE_HIGHLIGHT_COMPRESS and hi_clip > HI_CLIP_THR:
            out = highlight_compress(out); decisions["hicomp"] = True

    # ë””ë²„ê·¸ ë¬¸ìì—´
    y_in, hi_in, sharp_in, rg_in, bg_in   = measure_metrics(bgr)
    y_out, hi_out, sharp_out, rg_out, bg_out = measure_metrics(out)
    overlay = (f"GLOBAL_ENH:{global_enh_flag}  WB:{decisions['wb']} ENH:{decisions['enh']} "
               f"SH:{decisions['sharpen']} HI:{decisions['hicomp']}  "
               f"Y_in:{y_in:.1f}/out:{y_out:.1f}  S:{sharp_out:.0f}  R/G:{rg_out:.2f} B/G:{bg_out:.2f}")
    return out, overlay, decisions

# =========================
# 3) GT ê¸°ì¤€ í•´ìƒë„ ì½ê¸°
# =========================
_ref_cap = cv2.VideoCapture(ref_video_path)
REF_W = int(_ref_cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)
REF_H = int(_ref_cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)
_ref_cap.release()
if REF_W == 0 or REF_H == 0:
    raise RuntimeError("GT ê¸°ì¤€(ref) ì˜ìƒì˜ í•´ìƒë„ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ref_video_path í™•ì¸!")

def map_point_to_ref(cx, cy, cur_w, cur_h, ref_w, ref_h):
    rx = int(round(cx * (ref_w / max(1, cur_w))))
    ry = int(round(cy * (ref_h / max(1, cur_h))))
    return rx, ry

def map_bbox_to_ref(b, cur_w, cur_h, ref_w, ref_h):
    x1, y1, x2, y2 = b
    sx = ref_w / max(1, cur_w); sy = ref_h / max(1, cur_h)
    return [int(round(x1 * sx)), int(round(y1 * sy)),
            int(round(x2 * sx)), int(round(y2 * sy))]

print(f"[GT ê¸°ì¤€ í•´ìƒë„] {REF_W} x {REF_H}")

# =========================
# 4) ì˜ìƒ ë°ê¸° ì‚¬ì „ ì ê²€
# =========================
def video_brightness_probe(path, sample_frames=60, y_thr=VIDEO_Y_MEAN_THR):
    cap = cv2.VideoCapture(path)
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
    idxs = np.linspace(0, max(0, total-1), num=min(sample_frames, max(1, total)), dtype=int)

    ys, p75s, hi_clips = [], [], []
    for i in idxs:
        cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))
        ret, frame = cap.read()
        if not ret: continue
        roi = central_roi(frame)
        y = cv2.cvtColor(roi, cv2.COLOR_BGR2YCrCb)[:, :, 0].astype(np.float32)
        ys.append(float(np.mean(y)))
        p75s.append(float(np.percentile(y, 75)))
        hi_clips.append(float((y >= 250).mean()))
    cap.release()

    if not ys:
        return {"level": "unknown", "y_mean": None}

    y_mean   = float(np.mean(ys))
    y_median = float(np.median(ys))
    y_p75    = float(np.median(p75s))
    hi       = float(np.mean(hi_clips))

    is_dark = (y_median < y_thr and y_p75 < (y_thr + 30))
    if hi > 0.03:  # ê³¼ë…¸ì¶œ ë§ìœ¼ë©´ darkë¡œ ë³´ì§€ ì•ŠìŒ
        is_dark = False

    print(f"[ProbeDbg] mean={y_mean:.1f} med={y_median:.1f} p75={y_p75:.1f} hi={hi:.3f}")
    return {"level": "dark" if is_dark else "normal", "y_mean": y_mean}

probe = video_brightness_probe(video_path, sample_frames=60)
GLOBAL_ENH = (probe["level"] == "dark")
if FORCE_GLOBAL_ENH is True:
    GLOBAL_ENH = True
elif FORCE_GLOBAL_ENH is False:
    GLOBAL_ENH = False
print(f"[Video Brightness] level={probe['level']}  y_mean={probe['y_mean']}  -> GLOBAL_ENH={GLOBAL_ENH}")

# =========================
# 5) ë¹„ë””ì˜¤ IO
# =========================
cap = cv2.VideoCapture(video_path)
if not cap.isOpened():
    raise RuntimeError("âŒ ì˜ìƒ íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨")

width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps    = cap.get(cv2.CAP_PROP_FPS) or 25
fourcc = cv2.VideoWriter_fourcc(*('m','p','4','v'))
out         = cv2.VideoWriter(save_video_path, fourcc, fps, (width, height))
det_writer  = cv2.VideoWriter(enh_video_path, fourcc, fps, (width, height))  # ê²€ì¶œ/ì¸ì‹ ì…ë ¥(ë¯¸ë¦¬ë³´ê¸°)

# =========================
# 6) ì¶”ë¡  ë£¨í”„ + í‰ê°€ + ìë™ GT JSON
# =========================
frame_idx = 0

# ì‚¬ëŒ ë‹¨ìœ„ í˜¼ë™í–‰ë ¬
per_tp = per_fp = per_tn = per_fn = 0
per_total = 0
per_missed = 0
per_extra  = 0

def count_person(res):
    if res.boxes is None: return 0
    cls = res.boxes.cls.cpu().numpy().astype(int)
    return int((cls == 0).sum())

print("â–¶ ì˜ìƒ ê¸°ë°˜ ëª¨ìì´í¬ ì‹œì‘...")

while True:
    ret, frame = cap.read()
    if not ret: break

    cv2.imwrite(os.path.join(before_frame_dir, f"frame_{frame_idx:03d}.jpg"), frame)
    orig = frame.copy()

    # (1) ê²€ì¶œ/ì¸ì‹ ì…ë ¥ í”„ë ˆì„ ë§Œë“¤ê¸°
    frame_det, overlay_text, decisions = build_det_input(frame, GLOBAL_ENH)

    # ë¯¸ë¦¬ë³´ê¸°(ì˜¤ë²„ë ˆì´)
    frame_det_vis = frame_det.copy()
    cv2.rectangle(frame_det_vis, (8, 8), (8+1000, 8+32), (0,0,0), -1)
    cv2.putText(frame_det_vis, overlay_text, (16, 32),
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2, cv2.LINE_AA)
    det_writer.write(frame_det_vis)
    # cv2.imwrite(os.path.join(det_frame_dir, f"frame_{frame_idx:03d}.jpg"), frame_det_vis)

    person_idx = 0
    pred_objs_this_frame = []
    gt_objs = load_frame_json(frame_idx)
    gt_matched = [False] * (len(gt_objs) if gt_objs else 0)

    # (2) YOLO: ë³´ì • ì…ë ¥ ë¨¼ì €, ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¬ì‹œë„ (Patch C)
    results = yolo(frame_det, conf=0.5, iou=0.45)[0]
    if GLOBAL_ENH and count_person(results) == 0:
        # ì›ë³¸ìœ¼ë¡œ í´ë°±
        results_orig = yolo(orig, conf=0.5, iou=0.45)[0]
        if count_person(results_orig) > 0:
            results   = results_orig
            frame_det = orig  # ì´í›„ í¬ë¡­/ì„ë² ë”©ì€ ì´ í”„ë ˆì„ ê¸°ì¤€ìœ¼ë¡œ

    masks   = results.masks.data.cpu().numpy() if (results.masks is not None and results.masks.data is not None) else None
    classes = results.boxes.cls.cpu().numpy().astype(int) if results.boxes is not None else []
    boxes   = results.boxes.xyxy.cpu().numpy() if results.boxes is not None else []

    num_masks = len(masks) if masks is not None else 0
    cur_h, cur_w = frame.shape[:2]

    for idx, (cls, box) in enumerate(zip(classes, boxes)):
        if cls != 0:  # ì‚¬ëŒë§Œ
            continue

        x1, y1, x2, y2 = map(int, box)
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(cur_w, x2), min(cur_h, y2)
        if x2 <= x1 or y2 <= y1:
            person_idx += 1
            continue

        # (3) ArcFace: ì›ë³¸ ë¨¼ì €, ì‹¤íŒ¨ ì‹œ ë³´ì •ë³¸ í´ë°± (Patch A)
        crop_orig = orig[y1:y2, x1:x2]
        crop_det  = frame_det[y1:y2, x1:x2]

        face_name = f"frame_{frame_idx:03d}_p{person_idx}.jpg"
        cv2.imwrite(os.path.join(before_face_dir, face_name), crop_orig)

        need_mosaic = True
        sim_val = None
        try:
            # ì›ë³¸ ë¨¼ì €
            pil_orig = Image.fromarray(cv2.cvtColor(crop_orig, cv2.COLOR_BGR2RGB))
            face = mtcnn(pil_orig)
            used_enh_for_embed = False

            # ì›ë³¸ ì‹¤íŒ¨ + ì „ì—­ë³´ì • ì¼œì§ â†’ ë³´ì •ë³¸ì—ì„œ ì¬ì‹œë„
            if face is None and GLOBAL_ENH:
                pil_det = Image.fromarray(cv2.cvtColor(crop_det, cv2.COLOR_BGR2RGB))
                face = mtcnn(pil_det)
                used_enh_for_embed = face is not None

            if face is not None:
                with torch.no_grad():
                    emb = resnet(face.unsqueeze(0).to(device))[0].cpu().numpy()
                sim_val = cos_sim(centroid, emb)
                thr = THRESHOLD_ENH if used_enh_for_embed else THRESHOLD
                need_mosaic = (sim_val < thr)
            else:
                need_mosaic = True
        except Exception:
            need_mosaic = True
            sim_val = None

        # (4) ëª¨ìì´í¬ ì ìš© (ì›ë³¸ frameì—)
        if need_mosaic:
            if masks is not None and idx < num_masks:
                mosaic_mask(frame, masks[idx])
            else:
                roi = frame[y1:y2, x1:x2]
                if roi.size > 0:
                    small = cv2.resize(roi, (max(1, (x2-x1)//10), max(1, (y2-y1)//10)))
                    roi_m = cv2.resize(small, (x2-x1, y2-y1), interpolation=cv2.INTER_NEAREST)
                    frame[y1:y2, x1:x2] = roi_m

        # ì €ì¥
        crop_after = frame[y1:y2, x1:x2]
        cv2.imwrite(os.path.join(after_face_dir, face_name), crop_after)

        # ìë™ ì˜ˆì¸¡ JSON ê¸°ë¡
        bbox_cur = [int(x1), int(y1), int(x2), int(y2)]
        bbox_ref = map_bbox_to_ref(bbox_cur, cur_w, cur_h, REF_W, REF_H)
        pred_objs_this_frame.append({
            "bbox": bbox_cur,
            "bbox_ref": bbox_ref,
            "label": int(1 if need_mosaic else 0),   # 0=ë“±ë¡ì, 1=ë¹„ë“±ë¡ì
            "sim": None if sim_val is None else float(sim_val),
            "used_global_enh": bool(GLOBAL_ENH),
            "used_wb": bool(GLOBAL_ENH and USE_WB),
            "used_sharpen": False,  # decisionsëŠ” frame_det ê¸°ì¤€ì´ë¼ ë‹¨ì¼ í”Œë˜ê·¸ë¡œ ê¸°ë¡
            "used_hicomp":  False
        })

        # (5) ì‚¬ëŒ ë‹¨ìœ„ í‰ê°€(ì¤‘ì‹¬ì  ë§¤ì¹­; ì¢Œí‘œ refë¡œ ìŠ¤ì¼€ì¼)
        cx = (x1 + x2) // 2
        cy = (y1 + y2) // 2
        cx_ref, cy_ref = map_point_to_ref(cx, cy, cur_w, cur_h, REF_W, REF_H)

        matched = False
        if gt_objs:
            for gi, g in enumerate(gt_objs):
                gb = g.get("bbox"); gl = g.get("label")
                if gb is None or gl is None or gt_matched[gi]:
                    continue
                if center_in_bbox(cx_ref, cy_ref, gb):
                    matched = True
                    gt_matched[gi] = True
                    gt_label  = int(gl)
                    pred_label = 1 if need_mosaic else 0
                    if gt_label == 1 and pred_label == 1: per_tp += 1
                    elif gt_label == 1 and pred_label == 0: per_fn += 1
                    elif gt_label == 0 and pred_label == 1: per_fp += 1
                    else: per_tn += 1
                    per_total += 1
                    break
        if not matched:
            per_extra += 1

        person_idx += 1

    # ìë™ ì˜ˆì¸¡ JSON ì €ì¥
    with open(os.path.join(auto_pred_dir, f"frame_{frame_idx:03d}.json"), "w", encoding="utf-8") as f:
        json.dump({"image": f"frame_{frame_idx:03d}.jpg", "objects": pred_objs_this_frame},
                  f, ensure_ascii=False, indent=2)

    # ë§¤ì¹­ë˜ì§€ ì•Šì€ GT(ê²€ì¶œ ì‹¤íŒ¨)
    if gt_objs:
        for gi, _ in enumerate(gt_objs):
            if not gt_matched[gi]:
                per_missed += 1

    # ë¯¼ê° í…ìŠ¤íŠ¸ ë¸”ëŸ¬: ì¸ì‹ì€ frame_detì—ì„œ, ì ìš©ì€ ì›ë³¸ frameì—
    try:
        texts = ocr.readtext(frame_det)
        for (bbox, text, conf) in texts:
            if is_sensitive_text(text):
                frame = blur_polygon(frame, bbox)
    except Exception:
        pass

    # í”„ë ˆì„ ì €ì¥ + ë¹„ë””ì˜¤ ê¸°ë¡
    cv2.imwrite(os.path.join(after_frame_dir, f"frame_{frame_idx:03d}.jpg"), frame)
    out.write(frame)

    frame_idx += 1

cap.release()
out.release()
det_writer.release()

# =========================
# 7) ì‚¬ëŒ ë‹¨ìœ„ ì„±ëŠ¥ í‰ê°€ (JSON GT ê¸°ì¤€)
# =========================
print("\nâœ… [ì‚¬ëŒ ë‹¨ìœ„ í‰ê°€ - ìˆ˜ë™ JSON GT ê¸°ì¤€]")
print(f" - ë§¤ì¹­ ì„±ê³µ ê±´ìˆ˜: {per_total}")
print(f" - GT ìˆì—ˆëŠ”ë° ë§¤ì¹­ ì‹¤íŒ¨(ê²€ì¶œX): {per_missed}")
print(f" - ê²€ì¶œì€ ìˆì—ˆëŠ”ë° GT ì—†ìŒ/ë§¤ì¹­ ì‹¤íŒ¨: {per_extra}")

if per_total > 0:
    acc = (per_tp + per_tn) / per_total
    precision = per_tp / (per_tp + per_fp) if (per_tp + per_fp) > 0 else 0.0  # ì–‘ì„±=ë¹„ë“±ë¡ì
    recall    = per_tp / (per_tp + per_fn) if (per_tp + per_fn) > 0 else 0.0
    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

    print(f" - TP(ë¹„ë“±ë¡ì ëª¨ìì´í¬ ì„±ê³µ): {per_tp}")
    print(f" - FP(ë“±ë¡ì ì˜¤ëª¨ìì´í¬): {per_fp}")
    print(f" - FN(ë¹„ë“±ë¡ì ë†“ì¹¨): {per_fn}")
    print(f" - TN(ë“±ë¡ì í†µê³¼ ì„±ê³µ): {per_tn}")
    print(f" - Accuracy : {acc:.3f}")
    print(f" - Precision: {precision:.3f}")
    print(f" - Recall   : {recall:.3f}")
    print(f" - F1-score : {f1:.3f}")
else:
    print(" - ë§¤ì¹­ëœ ìƒ˜í”Œì´ ì—†ì–´ ì‚¬ëŒ ë‹¨ìœ„ í‰ê°€ëŠ” ìƒëµë©ë‹ˆë‹¤.")

print(f"\nğŸ¯ ìë™ ì˜ˆì¸¡ JSON í´ë”: {auto_pred_dir}")
print(f"ğŸ¬ ê²°ê³¼ ë¹„ë””ì˜¤(ì›ë³¸ì— ëª¨ìì´í¬ ì ìš©): {save_video_path}")
print(f"ğŸ¬ ê²€ì¶œ/ì¸ì‹ ì…ë ¥ ì˜ìƒ(ë³´ì • ë¯¸ë¦¬ë³´ê¸°): {enh_video_path}")
print(f"ğŸ–¼  ì „/í›„ í”„ë ˆì„: {before_frame_dir} | {after_frame_dir}")
print(f"ğŸ™‚  ì „/í›„ ì–¼êµ´ í¬ë¡­: {before_face_dir} | {after_face_dir}")
print(f"ğŸ–¼  (ì˜µì…˜) ê²€ì¶œ/ì¸ì‹ ì…ë ¥ í”„ë ˆì„ í´ë”: {det_frame_dir}")

# =========================
# 8) ArcFace ìœ ì‚¬ë„ ë¶„í¬ (ì°¸ê³ ìš©)
# =========================
def get_similarity(pil_img):
    face = mtcnn(pil_img)
    if face is None:
        return None
    with torch.no_grad():
        emb = resnet(face.unsqueeze(0).to(device))[0].cpu().numpy()
    return cos_sim(centroid, emb)

before_sims, after_sims = [], []
face_files = sorted([f for f in os.listdir(before_face_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))])

print(f"\nâ–¶ ArcFace ìœ ì‚¬ë„ ë¶„ì„(ì–¼êµ´ í¬ë¡­) ì¤‘... ì´ {len(face_files)}ê°œ")
for file in face_files:
    try:
        img_b = Image.open(os.path.join(before_face_dir, file)).convert('RGB')
        img_a = Image.open(os.path.join(after_face_dir,  file)).convert('RGB')
        sb = get_similarity(img_b)
        sa = get_similarity(img_a)
        if sb is not None: before_sims.append(sb)
        if sa is not None: after_sims.append(sa)
    except Exception:
        continue

before_sims = np.array(before_sims, dtype=float)
after_sims  = np.array(after_sims, dtype=float)

if len(before_sims) > 0 and len(after_sims) > 0:
    plt.hist(before_sims, bins=30, alpha=0.5, label='Before Blur')
    plt.hist(after_sims,  bins=30, alpha=0.5, label='After Blur')
    plt.axvline(THRESHOLD, color='red', linestyle='--', label=f'Threshold ({THRESHOLD})')
    plt.xlabel('ArcFace Cosine Similarity')
    plt.ylabel('Frequency')
    plt.title('ArcFace Similarity Distribution (Face Crops)')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    print("\nâœ… [ArcFace ìœ ì‚¬ë„ í†µê³„] (ì°¸ê³ ìš©)")
    print(f" - í‰ê·  ìœ ì‚¬ë„ (ì „) : {before_sims.mean():.4f}")
    print(f" - í‰ê·  ìœ ì‚¬ë„ (í›„) : {after_sims.mean():.4f}")
    print(f" - í‰ê·  ê°ì†ŒëŸ‰     : {(before_sims.mean() - after_sims.mean()):.4f}")
else:
    print("\nâ„¹ï¸ ìœ ì‚¬ë„ ë¶„í¬ë¥¼ ê³„ì‚°í•  ì¶©ë¶„í•œ ì–¼êµ´ í¬ë¡­ì´ ì—†ìŠµë‹ˆë‹¤.")